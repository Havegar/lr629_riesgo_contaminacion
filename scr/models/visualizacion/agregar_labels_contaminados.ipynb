{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718f9f4b-042c-4321-b8bc-d72ecdb94445",
   "metadata": {},
   "source": [
    "## Agregacion de labels a datos de entidades contaminadas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8395682a-6424-4f86-96b5-de157f223ec2",
   "metadata": {},
   "source": [
    "En este notebook se agregaran los labels a las entidades tributarias contaminadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77385d02-3a20-4cd2-add8-0ace72452c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from pyspark.sql.types import StringType,TimestampType\n",
    "from pyspark_dist_explore import hist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1225481b-cc1b-4e45-9868-1f1992da6753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 19:25:40 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/04/15 19:25:40 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "Setting spark.hadoop.yarn.resourcemanager.principal to hvega.externo\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/15 19:25:40 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/04/15 19:25:40 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/04/15 19:25:40 WARN SparkConf: The configuration key 'spark.yarn.access.hadoopFileSystems' has been deprecated as of Spark 3.0 and may be removed in the future. Please use the new key 'spark.kerberos.access.hadoopFileSystems' instead.\n",
      "24/04/15 19:25:42 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:503)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:590)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/15 19:25:42 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:503)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:590)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/15 19:25:43 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:503)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:590)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/15 19:25:43 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:503)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:590)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/15 19:25:43 WARN HadoopFSDelegationTokenProvider: Token ABFS/IDBroker has not set up issue date properly. (provided: 0) Using current timestamp (1713209143883) as issue date instead. Consult token implementor to fix the behavior.\n",
      "24/04/15 19:25:43 WARN HadoopFSDelegationTokenProvider: Token ABFS/IDBroker has not set up issue date properly. (provided: 0) Using current timestamp (1713209143888) as issue date instead. Consult token implementor to fix the behavior.\n",
      "24/04/15 19:25:44 WARN HiveServer2CredentialProvider: Failed to get HS2 delegation token\n",
      "java.util.NoSuchElementException: spark.sql.hive.hiveserver2.jdbc.url\n",
      "\tat org.apache.spark.SparkConf.$anonfun$get$1(SparkConf.scala:245)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.SparkConf.get(SparkConf.scala:245)\n",
      "\tat com.hortonworks.spark.deploy.yarn.security.HiveServer2CredentialProvider.obtainDelegationTokens(HiveServer2CredentialProvider.scala:64)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.$anonfun$obtainDelegationTokens$2(HadoopDelegationTokenManager.scala:164)\n",
      "\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:213)\n",
      "\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n",
      "\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$obtainDelegationTokens(HadoopDelegationTokenManager.scala:162)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:226)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager$$anon$4.run(HadoopDelegationTokenManager.scala:224)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1899)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.obtainTokensAndScheduleRenewal(HadoopDelegationTokenManager.scala:224)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.org$apache$spark$deploy$security$HadoopDelegationTokenManager$$updateTokensTask(HadoopDelegationTokenManager.scala:198)\n",
      "\tat org.apache.spark.deploy.security.HadoopDelegationTokenManager.start(HadoopDelegationTokenManager.scala:123)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1(CoarseGrainedSchedulerBackend.scala:552)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.$anonfun$start$1$adapted(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.start(CoarseGrainedSchedulerBackend.scala:549)\n",
      "\tat org.apache.spark.scheduler.cluster.k8s.KubernetesClusterSchedulerBackend.start(KubernetesClusterSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:581)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/15 19:25:44 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:503)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:590)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/15 19:25:44 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:503)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:590)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/15 19:25:45 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:503)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:590)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/15 19:25:45 WARN HttpChannel: /jobs/\n",
      "java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.\n",
      "\tat org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:51)\n",
      "\tat org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:276)\n",
      "\tat org.apache.spark.ui.WebUI.$anonfun$attachPage$1(WebUI.scala:90)\n",
      "\tat org.apache.spark.ui.JettyUtils$$anon$1.doGet(JettyUtils.scala:81)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:503)\n",
      "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:590)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHolder.handle(ServletHolder.java:791)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$ChainEnd.doFilter(ServletHandler.java:1626)\n",
      "\tat org.apache.spark.ui.HttpSecurityFilter.doFilter(HttpSecurityFilter.scala:95)\n",
      "\tat org.sparkproject.jetty.servlet.FilterHolder.doFilter(FilterHolder.java:193)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler$Chain.doFilter(ServletHandler.java:1601)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:548)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1435)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)\n",
      "\tat org.sparkproject.jetty.servlet.ServletHandler.doScope(ServletHandler.java:501)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1350)\n",
      "\tat org.sparkproject.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
      "\tat org.sparkproject.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:763)\n",
      "\tat org.sparkproject.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:234)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.apache.spark.ui.ProxyRedirectHandler.handle(JettyUtils.scala:581)\n",
      "\tat org.sparkproject.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
      "\tat org.sparkproject.jetty.server.Server.handle(Server.java:516)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:388)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.dispatch(HttpChannel.java:633)\n",
      "\tat org.sparkproject.jetty.server.HttpChannel.handle(HttpChannel.java:380)\n",
      "\tat org.sparkproject.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
      "\tat org.sparkproject.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
      "\tat org.sparkproject.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
      "\tat org.sparkproject.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n",
      "\tat org.sparkproject.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:383)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:882)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1036)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/04/15 19:25:45 WARN ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName(\"Test\")  \\\n",
    "  .config(\"spark.yarn.access.hadoopFileSystems\",\"abfs://data@datalakesii.dfs.core.windows.net/\") \\\n",
    "  .config(\"spark.executor.memory\", \"24g\") \\\n",
    "  .config(\"spark.driver.memory\", \"12g\")\\\n",
    "  .config(\"spark.executor.cores\", \"12\") \\\n",
    "  .config(\"spark.executor.instances\", \"24\") \\\n",
    "  .config(\"spark.driver.maxResultSize\", \"12g\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "sc=spark.sparkContext\n",
    "sc.setLogLevel ('ERROR')\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf6b4db-0b26-4132-b3de-ad9e5bb18693",
   "metadata": {},
   "source": [
    "## Lectura de datos de contaminacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877dfef7-8500-49cf-aabd-08ba06d0a3b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:========================================>               (34 + 13) / 47]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 4336704|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.options(header=True,inferSchema=True,delimiter=\",\").csv(\"/home/cdsw/data/processed/contaminados_processed_iva_representante_total.csv\").createOrReplaceTempView(\"contaminados_total\")\n",
    "spark.sql('select * from contaminados_total').createOrReplaceTempView(\"contaminados\")\n",
    "spark.sql('select count(*) from contaminados_total').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99e458e-cc73-4f07-86f8-47891ad5ed21",
   "metadata": {},
   "source": [
    "## Agregacion de labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a905e60-3115-4bfd-987f-ca988b55b9fd",
   "metadata": {},
   "source": [
    "Junto con la contaminacion, agregaremos mas labels para cada nodo, con el fin de caracterizar cada grupo cuando se ejecuten los algoritmos de comunidad. \n",
    "\n",
    "1.-Valor de contaminacion, que se obtuvo de la propagacion.\n",
    "\n",
    "2.-Cuanto ha pagado el contribuyente (cÃ³digo 89 del f29) desde el 2020 a la fecha.\n",
    "\n",
    "3.-Cuanto compra o vende el contribuyente (total neto) desde el 202 a la fecha.\n",
    "\n",
    "4.-Informacion geografica referida a la oficina que corresponde a cada contribuyente. \n",
    "\n",
    "5.-Edad del contribuyente (numero de documentos o fecha de constitucion) medida en dias. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661a8c48-eba8-49d8-90e6-6389657690fc",
   "metadata": {},
   "source": [
    "## Cuanto ha pagado el contribuyente (codigo 89 del formulario 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4fa077e-4ee6-4888-82af-8ecc2c569018",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=====================================================> (194 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT CONT_RUT)|\n",
      "+------------------------+\n",
      "|                 2867844|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/DW/DW_TRN_F29_E\").createOrReplaceTempView(\"tmp_f29\")\n",
    "spark.sql('select count(distinct(CONT_RUT)) from tmp_f29').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ebbd2b-da80-44d3-91c5-84dd977e42fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------------------+----------------+\n",
      "|            CONT_RUT|F29_C_89|              fecha|TIVA_COD_VALIDEZ|\n",
      "+--------------------+--------+-------------------+----------------+\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2020-08-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2020-09-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2022-11-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2023-01-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2022-09-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2022-12-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2021-09-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2021-08-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2022-03-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2022-08-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2023-06-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2023-07-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2021-01-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2022-02-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2020-06-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2022-01-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2022-10-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2021-11-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2021-07-01 00:00:00|               1|\n",
      "|+++4/3jzUwtDPSSo3...|     0.0|2020-11-01 00:00:00|               1|\n",
      "+--------------------+--------+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#se utiliza codigo de validez=1 que corresponde a los formularios validos.\n",
    "spark.sql(\"select CONT_RUT,F29_C_89,CONCAT(CAST(F29_AGNO_MES_TRIBUTARIO_VO AS VARCHAR(10) ),'01') as fecha,TIVA_COD_VALIDEZ from tmp_f29 where TIVA_COD_VALIDEZ=1\").createOrReplaceTempView(\"tmp_f29\")\n",
    "df=spark.sql(\"select * from tmp_f29\")\n",
    "df=df.withColumn(\"fecha\",to_timestamp(col(\"fecha\").cast(\"string\"), \"yyyyMMdd\"))\n",
    "df.schema\n",
    "df.createOrReplaceTempView(\"tmp_f29\")\n",
    "spark.sql(\"select * from tmp_f29 where fecha >= TO_TIMESTAMP( '2020-01-01', 'yyyy-mm-dd' ) order by cont_rut asc\").createOrReplaceTempView(\"tmp_f29\")\n",
    "spark.sql(\"select * from tmp_f29\").show()\n",
    "spark.sql(\"select CONT_RUT,sum(F29_C_89) as total_pago_f29 from tmp_f29  group by CONT_RUT\").createOrReplaceTempView(\"tmp_f29\")\n",
    "#spark.sql(\"select count(distinct(CONT_RUT)) from tmp_f29 where total_pago_f29 is not null \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7604a887-9e25-4f49-9b8d-3a27ccec6a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select contaminados.cont_rut, score, total_pago_f29 from contaminados left join tmp_f29 on contaminados.cont_rut=tmp_f29.CONT_RUT\").createOrReplaceTempView(\"contaminados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "953f88d4-14e4-4a39-8146-ae3ea92c7879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"select count(*) from contaminados\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d5eed-914b-42ae-9546-c7989e35e20e",
   "metadata": {},
   "source": [
    "## Cuanto compra o vende el contribuyente (IVA_neto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc4966de-68b2-4e36-b186-838fb51f73d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 01e9018f-781e-4b51-92e0-6ac297bfee68\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select dhdr_rut_emisor,dhdr_rut_recep,dhdr_iva,dhdr_mnt_total,dhdr_fch_emis_int from dwbgdata.header_dte_consolidada_enc_sas_analitica where (dtdc_codigo ='33' or dtdc_codigo='34')\").createOrReplaceTempView(\"consolidada\")\n",
    "df=spark.sql(\"select * from consolidada\")\n",
    "df=df.withColumn(\"dhdr_fch_emis_int\",to_timestamp(col(\"dhdr_fch_emis_int\").cast(\"string\"), \"yyyyMMdd\"))\n",
    "df.createOrReplaceTempView(\"consolidada\")\n",
    "#spark.sql(\"select count(*) from contaminados\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c23a72-ad9d-472b-9310-e034261f29d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from consolidada where dhdr_fch_emis_int >= TO_TIMESTAMP( '2020-01-01', 'yyyy-mm-dd')\").createOrReplaceTempView(\"consolidada\")\n",
    "\n",
    "spark.sql(\"select dhdr_rut_emisor, sum(dhdr_iva) as iva_emitido from consolidada group by dhdr_rut_emisor\").createOrReplaceTempView(\"emisor\")\n",
    "spark.sql(\"select dhdr_rut_recep, sum(dhdr_iva) as iva_recibido from consolidada group by dhdr_rut_recep\").createOrReplaceTempView(\"receptor\")\n",
    "\n",
    "spark.sql(\"select * from emisor full outer join receptor on emisor.dhdr_rut_emisor=receptor.dhdr_rut_recep\").createOrReplaceTempView(\"consolidada\")\n",
    "spark.sql(\"select case when dhdr_rut_emisor is null then dhdr_rut_recep else dhdr_rut_emisor end as cont_rut, case when iva_emitido is null then 0 else iva_emitido end as iva_emitido, case when iva_recibido is null then 0 else iva_recibido end as iva_recibido from consolidada\").createOrReplaceTempView(\"consolidada\")\n",
    "\n",
    "spark.sql(\"select cont_rut, (iva_recibido-iva_emitido) as iva_neto from consolidada\").createOrReplaceTempView(\"consolidada\")\n",
    "#spark.sql(\"select count(*) from consolidada\").show()\n",
    "\n",
    "#Cuando se hace alguna factura, el emisor debe pagar el IVA. Cuando se recibe una factura, el receptor retiene el IVA. \n",
    "# por lo que el valor neto del iva recibido es iva_recibido-iva_emitido\n",
    "\n",
    "spark.sql(\"select contaminados.cont_rut as cont_rut, score, total_pago_f29,consolidada.iva_neto as IVA_neto from contaminados left join consolidada on contaminados.cont_rut=consolidada.CONT_RUT\").createOrReplaceTempView(\"contaminados\")\n",
    "\n",
    "#spark.sql(\"select count(*) from contaminados\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8dbf71-4fd1-4f2c-bc97-250b5b8c7e3d",
   "metadata": {},
   "source": [
    "## Informacion geografica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16545004-42c5-406b-b117-b646ad93355d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"select * from dw.dw_trn_negocios_e\").show()\n",
    "spark.sql(\"select CONT_RUT, UNOP_DES_REGIONAL,NEGO_FECHA_VIGENCIA from dw.dw_trn_negocios_e  left join dw.dw_dim_unidad_operativa on dw.dw_trn_negocios_e.UNOP_UNIDAD= dw.dw_dim_unidad_operativa.UNOP_UNIDAD\").createOrReplaceTempView(\"geo\")\n",
    "spark.sql(\"WITH RankedData AS (SELECT CONT_RUT, UNOP_DES_REGIONAL,NEGO_FECHA_VIGENCIA, ROW_NUMBER() OVER (PARTITION BY CONT_RUT ORDER BY NEGO_FECHA_VIGENCIA DESC) AS RowNum FROM geo TuTabla) SELECT CONT_RUT, UNOP_DES_REGIONAL,NEGO_FECHA_VIGENCIA FROM RankedData WHERE RowNum = 1\").createOrReplaceTempView(\"geo\")\n",
    "#spark.sql(\"select * from geo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc76e9d7-4134-405c-8e24-07b09adfd945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select  contaminados.cont_rut as cont_rut, score, total_pago_f29,IVA_neto,UNOP_DES_REGIONAL as unidad_regional from contaminados left join  geo on contaminados.cont_rut=geo.CONT_RUT\").createOrReplaceTempView(\"contaminados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a7776d-7efa-4308-95ff-5ba3c8a057fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"select count(*) from contaminados\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1674e43-3335-43b6-859d-9bcc9d03a58c",
   "metadata": {},
   "source": [
    "## Documentos emitidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a0f7336-74d9-48a1-b9c7-077b8c03e9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select dhdr_rut_emisor, count(*) as n_documentos from dwbgdata.header_dte_consolidada_enc_sas_analitica where (dtdc_codigo ='33' or dtdc_codigo='34') group by dhdr_rut_emisor\").createOrReplaceTempView(\"emitidos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb1063c1-9412-4b4b-af0f-f70d46572505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"select count(distinct(dhdr_rut_recep)) from  dwbgdata.header_dte_consolidada_enc_sas_analitica where (dtdc_codigo ='33' or dtdc_codigo='34')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a930e24f-9dea-470c-a11f-c3ceae1bfa71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#spark.sql(\"select count(*) from emitidos\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3d8cdec-84d0-4ab8-8e93-88fdf293c514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select  contaminados.cont_rut, score, total_pago_f29,IVA_neto,unidad_regional, n_documentos from contaminados left join emitidos on contaminados.cont_rut=emitidos.dhdr_rut_emisor\").createOrReplaceTempView(\"contaminados\")\n",
    "#spark.sql(\"select count(*) from contaminados\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe9dd2-f9d6-4c06-b3f7-37e4ebac9010",
   "metadata": {},
   "source": [
    "## Duracion de la vida del contribuyente (en dias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5c7a4e4-b8d2-4d85-8910-ffd7dcf1f0f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/DW/DW_TRN_CONTRIBUYENTES_E\").createOrReplaceTempView(\"contribuyente\")\n",
    "#spark.sql('select count(*) from contribuyente where (cont_fecha_constitucion_vo is null or cont_fecha_constitucion_vo> NOW())').show()\n",
    "#spark.sql('select count(*) from contribuyente where (cont_fecha_constitucion_vo is not null or cont_fecha_constitucion_vo<= NOW())').show()\n",
    "#spark.sql('select cont_rut, count(*) as c from contribuyente group by cont_rut').createOrReplaceTempView(\"contribuyente\")\n",
    "#spark.sql('select cont_rut, c from contribuyente order by c desc').show()\n",
    "spark.sql(\"select cont_rut,cont_fecha_constitucion_vo, case when (cont_fecha_constitucion_vo is null or cont_fecha_constitucion_vo> NOW()) then NULL else DATEDIFF( NOW(),cont_fecha_constitucion_vo) end as lifetime from dw.dw_trn_contribuyentes_e order by cont_fecha_constitucion_vo desc \").createOrReplaceTempView(\"aux\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf05eeba-4325-4d76-9824-40fa4075597b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=====================================================>(299 + 1) / 300]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+--------+\n",
      "|            cont_rut|cont_fecha_constitucion_vo|lifetime|\n",
      "+--------------------+--------------------------+--------+\n",
      "|xWHI3pWiVUy93+tLZ...|       2024-03-25 03:00:00|      21|\n",
      "|lxBxF+gczV1MOGIgZ...|       2024-03-25 03:00:00|      21|\n",
      "|y+/AAozWrn7mF8KLW...|       2024-03-25 03:00:00|      21|\n",
      "|3itfDixx663/fPkHj...|       2024-03-25 03:00:00|      21|\n",
      "|HIJn1q5OKNDG70MIc...|       2024-03-25 03:00:00|      21|\n",
      "|pYndYDFhoDRPw3nUU...|       2024-03-25 03:00:00|      21|\n",
      "|5bpfGujgG2FooWCGW...|       2024-03-25 03:00:00|      21|\n",
      "|IPBKjLZju85eOmyX2...|       2024-03-25 03:00:00|      21|\n",
      "|FtzmlZ+LHfnXfWz5Z...|       2024-03-25 03:00:00|      21|\n",
      "|YmY0VS/Yh58Ekjjf/...|       2024-03-25 03:00:00|      21|\n",
      "|uujQqyoN8AqFDf7Rl...|       2024-03-25 03:00:00|      21|\n",
      "|RzxB+Jyh0Hj8mh3MB...|       2024-03-25 03:00:00|      21|\n",
      "|IUh3ah7y1gdpzyYD5...|       2024-03-25 03:00:00|      21|\n",
      "|Ar+iHTwh1V//K2wW4...|       2024-03-25 03:00:00|      21|\n",
      "|0B5pN08e2ZaOwVNWD...|       2024-03-25 03:00:00|      21|\n",
      "|A4qpAvMrvGcv0fdga...|       2024-03-25 03:00:00|      21|\n",
      "|P0n/a5v55Rv/kP/o9...|       2024-03-25 03:00:00|      21|\n",
      "|REUPBsHdoqQAw8StJ...|       2024-03-25 03:00:00|      21|\n",
      "|yjxQccDDmy+/Yi8wr...|       2024-03-25 03:00:00|      21|\n",
      "|+4gAEEyIJCf0gYx9E...|       2024-03-25 03:00:00|      21|\n",
      "+--------------------+--------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from aux where lifetime is not null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58fa112e-e140-45cf-8ee6-d54ed49e9cfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select cont_rut, case when (cont_fecha_constitucion_vo is null or cont_fecha_constitucion_vo> NOW()) then NULL else DATEDIFF( NOW(),cont_fecha_constitucion_vo) end as lifetime from dw.dw_trn_contribuyentes_e order by cont_fecha_constitucion_vo desc \").createOrReplaceTempView(\"lifetime\")\n",
    "spark.sql(\"select  contaminados.cont_rut, score,  total_pago_f29,IVA_neto,unidad_regional, n_documentos,lifetime from contaminados left join lifetime on contaminados.cont_rut=lifetime.cont_rut\").createOrReplaceTempView(\"contaminados\")\n",
    "#spark.sql(\"select count(*) from contaminados where lifetime is not null\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36813cbf-1ada-4d5a-adac-37bbb15cf742",
   "metadata": {},
   "source": [
    "## Contaminacion previa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7ad1e98-46c5-45cb-a4d2-d9427c1b0181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.options(header=True,inferSchema=True,delimiter=\",\").csv(\"/home/cdsw/data/processed/contaminados.csv\").createOrReplaceTempView(\"contaminados_inicio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ad78a23-920c-4a6c-8ba9-03523a1266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"select count(*) from contaminados_inicio\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d33df7b-094d-48bf-97e3-44af80d6e404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select contaminados.cont_rut, contaminados.score,  total_pago_f29,IVA_neto,unidad_regional, n_documentos,lifetime , case when contaminados_inicio.cont_rut is null then 0 else 1 end as alerta_inicial from contaminados left join contaminados_inicio on contaminados.cont_rut=contaminados_inicio.cont_rut\").createOrReplaceTempView(\"contaminados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54646bfa-352c-43c5-930c-2684d25bb132",
   "metadata": {},
   "source": [
    "## Guardamos la data previo a la busqueda de comunidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2578d19-399e-4a33-905f-c301fbe1df32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]]]]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "contaminados=spark.sql(\"select * from contaminados\")\n",
    "contaminados.write.mode('overwrite').format(\"parquet\").save(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/lr-629/propuesta_f29_comunidades/data_contaminados_with_labels\")\n",
    "\n",
    "contaminados=contaminados.toPandas()\n",
    "#guardamos los valores de contaminacion con las columnas adicionales\n",
    "contaminados.to_csv('/home/cdsw/data/processed/contaminados_processed_iva_representante_total_with_labels.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ccb98b-6be5-4e33-9672-3d97849bc743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
